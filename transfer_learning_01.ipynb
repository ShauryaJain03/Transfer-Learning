{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNC490uKKT+Ig24yqzOZEge",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShauryaJain03/Transfer-Learning/blob/main/transfer_learning_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fine-tuning**"
      ],
      "metadata": {
        "id": "wGLg-QEMhsgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In fine-tuning transfer learning the pre-trained model weights from another model are unfrozen and tweaked during to better suit your own data.\n",
        "\n",
        "For feature extraction transfer learning, you may only train the top 1-3 layers of a pre-trained model with your own data, in fine-tuning transfer learning, you might train 1-3+ layers of a pre-trained model (where the '+' indicates that many or all of the layers could be trained).\n",
        "\n",
        " Feature extraction transfer learning vs. fine-tuning transfer learning. The main difference between the two is that in fine-tuning, more layers of the pre-trained model get unfrozen and tuned on custom data. This fine-tuning usually takes more data than feature extraction to be effective"
      ],
      "metadata": {
        "id": "bKCHXZTwh3mm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to go through the follow with TensorFlow:\n",
        "\n",
        "- Introduce fine-tuning, a type of transfer learning to modify a pre-trained model to be more suited to your data\n",
        "Using the Keras Functional API (a differnt way to build models in Keras)\n",
        "\n",
        "- Using a smaller dataset to experiment faster (e.g. 1-10% of training samples of 10 classes of food)\n",
        "\n",
        "- Data augmentation (how to make your training dataset more diverse without adding more data)\n",
        "\n",
        "Running a series of modelling experiments on our Food Vision data\n",
        "\n",
        "0. Model 0: a transfer learning model using the Keras Functional API\n",
        "1. Model 1: a feature extraction transfer learning model on 1% of the data with data augmentation\n",
        "2. Model 2: a feature extraction transfer learning model on 10% of the data with data augmentation\n",
        "3. Model 3: a fine-tuned transfer learning model on 10% of the data\n",
        "4. Model 4: a fine-tuned transfer learning model on 100% of the data\n",
        "5. Introduce the ModelCheckpoint callback to save intermediate training results\n",
        "6. Compare model experiments results using TensorBoard"
      ],
      "metadata": {
        "id": "T6r8N1kMsr3W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkqPI7Axfv0H",
        "outputId": "110da5ef-084a-4732-9e7d-41428d265127"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-14 11:39:30--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10246 (10K) [text/plain]\n",
            "Saving to: ‘helper_functions.py’\n",
            "\n",
            "helper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-01-14 11:39:30 (80.6 MB/s) - ‘helper_functions.py’ saved [10246/10246]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Get helper_functions.py script from course GitHub\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
        "\n",
        "# Import helper functions we're going to use\n",
        "from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, walk_through_dir"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 10% of the data of the 10 classes\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
        "\n",
        "unzip_data(\"10_food_classes_10_percent.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U30rjhVUuBZM",
        "outputId": "b6113f95-4f3c-4f7b-afe0-1dab26cf6314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-14 11:39:32--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.211.207, 142.251.162.207, 173.194.212.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.211.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 168546183 (161M) [application/zip]\n",
            "Saving to: ‘10_food_classes_10_percent.zip’\n",
            "\n",
            "10_food_classes_10_ 100%[===================>] 160.74M   101MB/s    in 1.6s    \n",
            "\n",
            "2024-01-14 11:39:33 (101 MB/s) - ‘10_food_classes_10_percent.zip’ saved [168546183/168546183]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Walk through 10 percent data directory and list number of files\n",
        "walk_through_dir(\"10_food_classes_10_percent\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKKqDxvMuS12",
        "outputId": "49bd5bdd-88c9-4408-adaf-2f3226810473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 directories and 0 images in '10_food_classes_10_percent'.\n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/test'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/hamburger'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_wings'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ramen'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/sushi'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/grilled_salmon'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_curry'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/steak'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/pizza'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ice_cream'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/fried_rice'.\n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/train'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/hamburger'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_wings'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ramen'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/sushi'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/grilled_salmon'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_curry'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/steak'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/pizza'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ice_cream'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/fried_rice'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and test directories\n",
        "train_dir = \"10_food_classes_10_percent/train/\"\n",
        "test_dir = \"10_food_classes_10_percent/test/\""
      ],
      "metadata": {
        "id": "qyLCGPdPuwP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data inputs\n",
        "import tensorflow as tf\n",
        "IMG_SIZE = (224, 224) # define image size\n",
        "train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir,\n",
        "                                                                            image_size=IMG_SIZE,\n",
        "                                                                            label_mode=\"categorical\", # what type are the labels?\n",
        "                                                                            batch_size=32) # batch_size is 32 by default, this is generally a good number\n",
        "test_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=test_dir,\n",
        "                                                                           image_size=IMG_SIZE,\n",
        "                                                                           label_mode=\"categorical\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-m4O9Wlz3yA",
        "outputId": "ec48ba0c-e4bf-486b-d646-3470d2e1e409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 750 files belonging to 10 classes.\n",
            "Found 2500 files belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For now, the main parameters we're concerned about in the image_dataset_from_directory() funtion are:\n",
        "\n",
        "directory - the filepath of the target directory we're loading images in from.\n",
        "\n",
        "image_size - the target size of the images we're going to load in (height, width).\n",
        "\n",
        "batch_size - the batch size of the images we're going to load in. For example if the batch_size is 32 (the default), batches of 32 images and labels at a time will be passed to the model."
      ],
      "metadata": {
        "id": "Id3MTmBZ0kLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_10_percent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yc1txm080mxR",
        "outputId": "edb2d32d-c83c-4548-f441-caa24127e606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(None, 224, 224, 3) refers to the tensor shape of our images where None is the batch size, 224 is the height (and width) and 3 is the color channels (red, green, blue).\n",
        "\n",
        "(None, 10) refers to the tensor shape of the labels where None is the batch size and 10 is the number of possible labels (the 10 different food classes).\n",
        "\n",
        "Both image tensors and labels are of the datatype tf.float32."
      ],
      "metadata": {
        "id": "VhvhVoQO1470"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_10_percent.class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7-xK50pA0rI",
        "outputId": "1814c4b2-b52e-4671-cfed-e9485164bcfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['chicken_curry',\n",
              " 'chicken_wings',\n",
              " 'fried_rice',\n",
              " 'grilled_salmon',\n",
              " 'hamburger',\n",
              " 'ice_cream',\n",
              " 'pizza',\n",
              " 'ramen',\n",
              " 'steak',\n",
              " 'sushi']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for images,labels in train_data_10_percent.take(1):\n",
        "  print(images,labels)"
      ],
      "metadata": {
        "id": "S3dPrvxqBdOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how the image arrays come out as tensors of pixel values where as the labels come out as one-hot encodings (e.g. [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] for hamburger)."
      ],
      "metadata": {
        "id": "3CI69tBlO5Dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images.shape , labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qR9ccvFKPfWB",
        "outputId": "0ab8b238-ba9a-4397-bec0-435b2f6c56d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([32, 224, 224, 3]), TensorShape([32, 10]))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model 0: Building a transfer learning model using the Keras Functional API**\n",
        "\n",
        "1. Instantiate a pre-trained base model object by choosing a target model such as EfficientNetV2B0 from tf.keras.applications.efficientnet_v2, setting the include_top parameter to False (we do this because we're going to create our own top, which are the output layers for the model).\n",
        "\n",
        "2. Set the base model's trainable attribute to False to freeze all of the weights in the pre-trained model.\n",
        "\n",
        "3. Define an input layer for our model, for example, what shape of data should our model expect?\n",
        "\n",
        "[Optional] Normalize the inputs to our model if it requires. Some computer vision models such as ResNetV250 require their inputs to be between 0 & 1.\n",
        "\n",
        "4. Pass the inputs to the base model.\n",
        "\n",
        "5. Pool the outputs of the base model into a shape compatible with the output activation layer (turn base model output tensors into same shape as label tensors). This can be done using tf.keras.layers.GlobalAveragePooling2D() or tf.keras.layers.GlobalMaxPooling2D() though the former is more common in practice.\n",
        "\n",
        "6. Create an output activation layer using tf.keras.layers.Dense() with the appropriate activation function and number of neurons.\n",
        "7. Combine the inputs and outputs layer into a model using tf.keras.Model().\n",
        "8. Compile the model using the appropriate loss function and choose of optimizer.\n",
        "9. Fit the model for desired number of epochs and with necessary callbacks (in our case, we'll start off with the TensorBoard callback)."
      ],
      "metadata": {
        "id": "NjhiwDHFR3bI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create base model with tf.keras.applications\n",
        "base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False)\n",
        "# 2. Freeze the base model (so the pre-learned patterns remain)\n",
        "base_model.trainable = False\n",
        "\n",
        "# 3. Create inputs into the base model\n",
        "inputs = tf.keras.layers.Input(shape=(224, 224, 3), name=\"input_layer\")\n",
        "\n",
        "# 4. If using ResNet50V2, add this to speed up convergence, remove for EfficientNetV2\n",
        "# x = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)(inputs)\n",
        "\n",
        "# 5. Pass the inputs to the base_model (note: using tf.keras.applications, EfficientNetV2 inputs don't have to be normalized)\n",
        "x = base_model(inputs)\n",
        "# Check data shape after passing it to base_model\n",
        "print(f\"Shape after base_model: {x.shape}\")\n",
        "\n",
        "# 6. Average pool the outputs of the base model (aggregate all the most important information, reduce number of computations)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
        "print(f\"After GlobalAveragePooling2D(): {x.shape}\")\n",
        "\n",
        "# 7. Create the output activation layer\n",
        "outputs = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x)\n",
        "\n",
        "# 8. Combine the inputs with the outputs into a model\n",
        "model_0 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# 9. Compile the model\n",
        "model_0.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# 10. Fit the model (we use less steps for validation so it's faster)\n",
        "history_10_percent = model_0.fit(train_data_10_percent,\n",
        "                                 epochs=5,\n",
        "                                 steps_per_epoch=len(train_data_10_percent),\n",
        "                                 validation_data=test_data_10_percent,\n",
        "                                 # Go through less of the validation data so epochs are faster (we want faster experiments!)\n",
        "                                 validation_steps=int(0.25 * len(test_data_10_percent)),\n",
        "                                 # Track our model's training logs for visualization later\n",
        "                                 callbacks=[create_tensorboard_callback(\"transfer_learning\", \"10_percent_feature_extract\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3DzgH1KPlVb",
        "outputId": "1bfe553d-815d-4038-bc17-8998e49e114d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape after base_model: (None, 7, 7, 1280)\n",
            "After GlobalAveragePooling2D(): (None, 1280)\n",
            "Saving TensorBoard log files to: transfer_learning/10_percent_feature_extract/20240114-113944\n",
            "Epoch 1/5\n",
            "24/24 [==============================] - 97s 4s/step - loss: 1.9054 - accuracy: 0.4067 - val_loss: 1.3145 - val_accuracy: 0.7286\n",
            "Epoch 2/5\n",
            "24/24 [==============================] - 87s 4s/step - loss: 1.1432 - accuracy: 0.7493 - val_loss: 0.8916 - val_accuracy: 0.8174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.evaluate(test_data_10_percent)"
      ],
      "metadata": {
        "id": "M76QnVaqm5GD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer_number,layer in enumerate(base_model.layers):\n",
        "  print(layer_number , layer.name)"
      ],
      "metadata": {
        "id": "bi-0oQEhnRrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.summary()"
      ],
      "metadata": {
        "id": "wvGG1cZonqVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see how each of the different layers have a certain number of parameters each. Since we are using a pre-trained model, you can think of all of these parameters are patterns the base model has learned on another dataset. And because we set base_model.trainable = False, these patterns remain as they are during training (they're frozen and don't get updated)"
      ],
      "metadata": {
        "id": "vH0vlEHEn7j_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.summary()"
      ],
      "metadata": {
        "id": "zaVUlW1An8Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot loss crves\n",
        "plot_loss_curves(history_10_percent)"
      ],
      "metadata": {
        "id": "7SguscZtoUJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GETTING FEATURE VECTOR FROM A TRAINED MODEL\n",
        "\n",
        "The tf.keras.layers.GlobalAveragePooling2D() layer transforms a 4D tensor into a 2D tensor by averaging the values across the inner-axes."
      ],
      "metadata": {
        "id": "KV_md-gSqzwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape=(1,4,4,3)\n",
        "input_tensor=tf.random.normal(input_shape)\n",
        "\n",
        "#global-average pooled tensor\n",
        "pooled_tensor=tf.keras.layers.GlobalAveragePooling2D()(input_tensor)\n",
        "pooled_tensor"
      ],
      "metadata": {
        "id": "j6TQWuDBq7N-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see the tf.keras.layers.GlobalAveragePooling2D() layer condensed the input tensor from shape (1, 4, 4, 3) to (1, 3). It did so by averaging the input_tensor across the middle two axe\n",
        "\n",
        "#### **Doing this not only makes the output of the base model compatible with the input shape requirement of our output layer (tf.keras.layers.Dense()), it also condenses the information found by the base model into a lower dimension feature vector.**\n",
        "\n",
        "#### **Note: One of the reasons feature extraction transfer learning is named how it is is because what often happens is a pretrained model outputs a feature vector (a long tensor of numbers, in our case, this is the output of the tf.keras.layers.GlobalAveragePooling2D() layer) which can then be used to extract patterns out of**"
      ],
      "metadata": {
        "id": "cV0nUFdwujHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model 1: Use feature extraction transfer learning on 1% of the training data with data augmentation.**"
      ],
      "metadata": {
        "id": "GOUr2aApukH_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "instead of implementing data augmentation in the ImageDataGenerator class as we have previously, we're going to build it right into the model using the tf.keras.layers module."
      ],
      "metadata": {
        "id": "zHIr2B4Hwzbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_1_percent.zip\n",
        "import zipfile\n",
        "zip_ref=zipfile.ZipFile(\"10_food_classes_1_percent.zip\")\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "L79dssj8vNnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir_1_percent=\"10_food_classes_1_percent/train\"\n",
        "test_dir=\"10_food_classes_1_percent/test\""
      ],
      "metadata": {
        "id": "zVpLKtR724VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for dirpath,dirnames,filenames in os.walk(\"/content/10_food_classes_1_percent\"):\n",
        "  print(f\"Found {len(dirnames)} directories and {len(filenames)} images in {dirpath}\")"
      ],
      "metadata": {
        "id": "boGZSywI33fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "img_size=(224,224)\n",
        "train_data_1_percent=tf.keras.preprocessing.image_dataset_from_directory(train_dir_1_percent,batch_size=32,image_size=img_size,label_mode=\"categorical\")\n",
        "test_data=tf.keras.preprocessing.image_dataset_from_directory(test_dir,batch_size=32,image_size=img_size,label_mode=\"categorical\")\n"
      ],
      "metadata": {
        "id": "5K15hyCi9BEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Adding data augmentation right into the model**\n",
        "Previously we've used the different parameters of the ImageDataGenerator class to augment our training images, this time we're going to build data augmentation right into the model.\n",
        "\n",
        "How?\n",
        "\n",
        "Using the tf.keras.layers module and creating a dedicated data augmentation layer.\n",
        "\n",
        "- Preprocessing of the images (augmenting them) happens on the GPU rather than on the CPU (much faster).\n",
        "Images are best preprocessed on the GPU where as text and structured data are more suited to be preprocessed on the CPU.\n",
        "\n",
        "- Image data augmentation only happens during training so we can still export our whole model and use it elsewhere. And if someone else wanted to train the same model as us, including the same kind of data augmentation, they could."
      ],
      "metadata": {
        "id": "JRktj78V_jHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data augmentation transformations we're going to use are:\n",
        "\n",
        "- tf.keras.layers.RandomFlip - flips image on horizontal or vertical axis.\n",
        "- tf.keras.layersRandomRotation - randomly rotates image by a specified amount.\n",
        "- tf.keras.layers.RandomZoom - randomly zooms into an image by specified amount.\n",
        "- tf.keras.layers.RandomHeight - randomly shifts image height by a specified amount.\n",
        "- tf.keras.layers.RandomWidth - randomly shifts image width by a specified amount.\n",
        "- tf.keras.layers.Rescaling - normalizes the image pixel values to be between 0 and 1, this is worth mentioning because it is required for some image models but since we're using tf.keras.applications.efficientnet_v2.EfficientNetV2B0, it's not required (the model pretrained model implements rescaling itself)."
      ],
      "metadata": {
        "id": "FmlW7YJiCdIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation=tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.2),\n",
        "    tf.keras.layers.RandomZoom(0.2),\n",
        "    tf.keras.layers.RandomHeight(0.2),\n",
        "    tf.keras.layers.RandomWidth(0.2),\n",
        "    # preprocessing.Rescaling(1./255) # keep for ResNet50V2, remove for EfficientNetV2B0\n",
        "],name=\"data_augmentation\")"
      ],
      "metadata": {
        "id": "jqLo6VRw_1h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View a random image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import os\n",
        "import random\n",
        "target_class = random.choice(train_data_1_percent.class_names) # choose a random class\n",
        "target_dir = \"10_food_classes_1_percent/train/\" + target_class # create the target directory\n",
        "random_image = random.choice(os.listdir(target_dir)) # choose a random image from target directory\n",
        "random_image_path = target_dir + \"/\" + random_image # create the choosen random image path\n",
        "img = mpimg.imread(random_image_path) # read in the chosen target image\n",
        "plt.imshow(img) # plot the target image\n",
        "plt.title(f\"Original random image from class: {target_class}\")\n",
        "plt.axis(False); # turn off the axes\n",
        "\n",
        "# Augment the image\n",
        "augmented_img = data_augmentation(tf.expand_dims(img, axis=0)) # data augmentation model requires shape (None, height, width, 3)\n",
        "plt.figure()\n",
        "plt.imshow(tf.squeeze(augmented_img)/255.) # requires normalization after augmentation\n",
        "plt.title(f\"Augmented random image from class: {target_class}\")\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "deG6kI_kDqf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup input shape and base model, freezing the base model layers\n",
        "input_shape = (224, 224, 3)\n",
        "base_model=tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False)\n",
        "\n",
        "#input layer\n",
        "inputs=tf.keras.layers.Input(shape=input_shape,name=\"input_layer\")\n",
        "\n",
        "# Add in data augmentation Sequential model as a layer\n",
        "x = data_augmentation(inputs)\n",
        "\n",
        "# Give base_model inputs (after augmentation) and don't train it\n",
        "x = base_model(x, training=False)\n",
        "\n",
        "# Pool output features of base model\n",
        "x = tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
        "\n",
        "# Put a dense layer on as the output\n",
        "outputs = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x)\n",
        "\n",
        "# Make a model with inputs and outputs\n",
        "model_1 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# compile the model\n",
        "model_1.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                optimizer=\"Adam\",\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "#fit the model\n",
        "history_1_percent = model_1.fit(train_data_1_percent,\n",
        "                    epochs=5,\n",
        "                    steps_per_epoch=len(train_data_1_percent),\n",
        "                    validation_data=test_data,\n",
        "                    validation_steps=int(0.25* len(test_data)), # validate for less steps\n",
        "                    # Track model training logs\n",
        "                    callbacks=[create_tensorboard_callback(\"transfer_learning\", \"1_percent_data_aug\")])"
      ],
      "metadata": {
        "id": "nj9HcoUrFk6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.summary()"
      ],
      "metadata": {
        "id": "z8xYEMONGxUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The important thing to remember is data augmentation only runs during training. So if we were to evaluate or use our model for inference (predicting the class of an image) the data augmentation layers will be automatically turned off."
      ],
      "metadata": {
        "id": "v4NekjpvK4pQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the test data\n",
        "results_1_percent_data_aug = model_1.evaluate(test_data)\n",
        "results_1_percent_data_aug\n"
      ],
      "metadata": {
        "id": "EZdChGvqK5Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How does the model go with a data augmentation layer with 1% of data\n",
        "plot_loss_curves(history_1_percent)"
      ],
      "metadata": {
        "id": "W8RpnNOALJaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MODEL 2 -  Feature extraction transfer learning with 10% of data and data augmentation**"
      ],
      "metadata": {
        "id": "oS99o2EO7Zz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir_10_percent=\"10_food_classes_10_percent/train/\"\n",
        "test_dir=\"10_food_classes_10_percent/test/\""
      ],
      "metadata": {
        "id": "hJrgLxr_823i"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup data inputs\n",
        "import tensorflow as tf\n",
        "IMG_SIZE = (224, 224)\n",
        "train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir_10_percent,\n",
        "                                                                            label_mode=\"categorical\",\n",
        "                                                                            image_size=IMG_SIZE)\n",
        "# Note: the test data is the same as the previous experiment, we could\n",
        "# skip creating this, but we'll leave this here to practice.\n",
        "test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n",
        "                                                                label_mode=\"categorical\",\n",
        "                                                                image_size=IMG_SIZE)"
      ],
      "metadata": {
        "id": "ESPBFtFQC4dN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "231d9079-bca7-4852-b0ac-203fb3682448"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 750 files belonging to 10 classes.\n",
            "Found 2500 files belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation=tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.2),\n",
        "    tf.keras.layers.RandomZoom(0.2),\n",
        "    tf.keras.layers.RandomHeight(0.2),\n",
        "    tf.keras.layers.RandomWidth(0.2),\n",
        "    # preprocessing.Rescaling(1./255) # keep for ResNet50V2, remove for EfficientNetV2B0\n",
        "],name=\"data_augmentation\")\n",
        "\n",
        "# Setup the input shape to our model\n",
        "input_shape = (224, 224, 3)\n",
        "# Create a frozen base model\n",
        "base_model=tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False)\n",
        "base_model.trainable=False\n",
        "\n",
        " # augment our training images\n",
        "inputs=tf.keras.layers.Input(shape=input_shape,name=\"input_layer\")\n",
        "\n",
        " # augment our training images\n",
        "x=data_augmentation(inputs)\n",
        "\n",
        "# pass augmented images to base model but keep it in inference mode, so batchnorm layers dont get updated\n",
        "x=base_model(x,training=False)\n",
        "\n",
        "x=tf.keras.layers.GlobalAveragePooling2D(name=\"global_pooling_layer\")(x)\n",
        "\n",
        "outputs=tf.keras.layers.Dense(10,activation=\"softmax\",name=\"output_layer\")(x)\n",
        "\n",
        "model_2=tf.keras.Model(inputs,outputs)\n",
        "\n",
        "# Compile\n",
        "model_2.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # use Adam optimizer with base learning rate\n",
        "              metrics=[\"accuracy\"])\n"
      ],
      "metadata": {
        "id": "y329vGCmEEOv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating a ModelCheckpoint callback**\n",
        "\n",
        "Our model is compiled and ready to be fit, so why haven't we fit it yet?\n",
        "\n",
        "Well, for this experiment we're going to introduce a new callback, the ModelCheckpoint callback.\n",
        "\n",
        "The ModelCheckpoint callback gives you the ability to save your model, as a whole in the SavedModel format or the weights (patterns) only to a specified directory as it trains.\n",
        "\n",
        "This is helpful if you think your model is going to be training for a long time and you want to make backups of it as it trains. It also means if you think your model could benefit from being trained for longer, you can reload it from a specific checkpoint and continue training from there.\n",
        "\n",
        "For example, say you fit a feature extraction transfer learning model for 5 epochs and you check the training curves and see it was still improving and you want to see if fine-tuning for another 5 epochs could help, you can load the checkpoint, unfreeze some (or all) of the base model layers and then continue training."
      ],
      "metadata": {
        "id": "Ld_8fedfKrWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path=\"ten_percent_model_checkpoints_weights/checkpoint.ckpt\"\n",
        "\n",
        "# Create a ModelCheckpoint callback that saves the model's weights only\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                         save_weights_only=True, # set to False to save the entire model\n",
        "                                                         save_best_only=True, # save only the best model weights instead of a model every epoch\n",
        "                                                         save_freq=\"epoch\", # save every epoch\n",
        "                                                         verbose=1)"
      ],
      "metadata": {
        "id": "CR7qKH0cKty1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SavedModel format saves a model's architecture, weights and training configuration all in one folder. It makes it very easy to reload your model exactly how it is elsewhere.\n",
        "\n",
        "However, if you do not want to share all of these details with others, you may want to save and share the weights only (these will just be large tensors of non-human interpretable numbers). If disk space is an issue, saving the weights only is faster and takes up less space than saving the whole model."
      ],
      "metadata": {
        "id": "tmTsS4cFRSDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Fit the model saving checkpoints every epoch\n",
        "initial_epochs = 5\n",
        "history_10_percent_data_aug = model_2.fit(train_data_10_percent,\n",
        "                                          epochs=initial_epochs,\n",
        "                                          validation_data=test_data,\n",
        "                                          validation_steps=int(0.25 * len(test_data)), # do less steps per validation (quicker)\n",
        "                                          callbacks=[\n",
        "                                                     checkpoint_callback])\n"
      ],
      "metadata": {
        "id": "FO9HetV-RVAQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0af66bb1-6b07-418f-c07b-5ca9bcd33a9d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "24/24 [==============================] - ETA: 0s - loss: 2.0192 - accuracy: 0.3520\n",
            "Epoch 1: val_loss improved from inf to 1.51081, saving model to ten_percent_model_checkpoints_weights/checkpoint.ckpt\n",
            "24/24 [==============================] - 152s 6s/step - loss: 2.0192 - accuracy: 0.3520 - val_loss: 1.5108 - val_accuracy: 0.6365\n",
            "Epoch 2/5\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.3887 - accuracy: 0.6773\n",
            "Epoch 2: val_loss improved from 1.51081 to 1.05492, saving model to ten_percent_model_checkpoints_weights/checkpoint.ckpt\n",
            "24/24 [==============================] - 98s 4s/step - loss: 1.3887 - accuracy: 0.6773 - val_loss: 1.0549 - val_accuracy: 0.7664\n",
            "Epoch 3/5\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.1064 - accuracy: 0.7120\n",
            "Epoch 3: val_loss improved from 1.05492 to 0.86546, saving model to ten_percent_model_checkpoints_weights/checkpoint.ckpt\n",
            "24/24 [==============================] - 94s 4s/step - loss: 1.1064 - accuracy: 0.7120 - val_loss: 0.8655 - val_accuracy: 0.7944\n",
            "Epoch 4/5\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9333 - accuracy: 0.7680\n",
            "Epoch 4: val_loss improved from 0.86546 to 0.73865, saving model to ten_percent_model_checkpoints_weights/checkpoint.ckpt\n",
            "24/24 [==============================] - 95s 4s/step - loss: 0.9333 - accuracy: 0.7680 - val_loss: 0.7386 - val_accuracy: 0.8191\n",
            "Epoch 5/5\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.8132 - accuracy: 0.7973\n",
            "Epoch 5: val_loss improved from 0.73865 to 0.63106, saving model to ten_percent_model_checkpoints_weights/checkpoint.ckpt\n",
            "24/24 [==============================] - 95s 4s/step - loss: 0.8132 - accuracy: 0.7973 - val_loss: 0.6311 - val_accuracy: 0.8322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_10_percent_data_aug=model_2.evaluate(test_data)"
      ],
      "metadata": {
        "id": "7rnsZLxfUr07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76877348-be77-4746-b809-c4d88a139147"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79/79 [==============================] - 155s 2s/step - loss: 0.6782 - accuracy: 0.8148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(history_10_percent_data_aug)"
      ],
      "metadata": {
        "id": "6f6PGOndU57p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at these, our model's performance with 10% of the data and data augmentation isn't as good as the model with 10% of the data without data augmentation (see model_0 results above), however the curves are trending in the right direction, meaning if we decided to train for longer, its metrics would likely improve.\n",
        "\n",
        "Since we checkpointed (is that a word?) our model's weights, we might as well see what it's like to load it back in. We'll be able to test if it saved correctly by evaluting it on the test data."
      ],
      "metadata": {
        "id": "3LX30_9IVUSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in saved model weights and evaluate model\n",
        "model_2.load_weights(checkpoint_path)\n",
        "loaded_weights_model_results=model_2.evaluate(test_data)"
      ],
      "metadata": {
        "id": "jPS1XzP7ZmFL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "b5e501d4-733f-47fa-9fe7-4bd8e60d7979"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 1/79 [..............................] - ETA: 2:55 - loss: 0.8793 - accuracy: 0.7188"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-f1f1f1d157c6>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load in saved model weights and evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mloaded_weights_model_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2294\u001b[0m                         ):\n\u001b[1;32m   2295\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2296\u001b[0;31m                             logs = test_function_runner.run_step(\n\u001b[0m\u001b[1;32m   2297\u001b[0m                                 \u001b[0mdataset_or_iterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2298\u001b[0m                                 \u001b[0mdata_handler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(self, dataset_or_iterator, data_handler, step, unused_shards)\u001b[0m\n\u001b[1;32m   4106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_or_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_shards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m         \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_or_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    878\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If the results from our native model and the loaded weights are the same, this should output True\n",
        "results_10_percent_data_aug == loaded_weights_model_results"
      ],
      "metadata": {
        "id": "nzsyRQSccawp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the above cell doesn't output True, it's because the numbers are close but not the exact same (due to how computers store numbers with degrees of precision).\n",
        "\n",
        "However, they should be very close..."
      ],
      "metadata": {
        "id": "uBR2olEZclMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Check to see if loaded model results are very close to native model results (should output True)\n",
        "np.isclose(np.array(results_10_percent_data_aug), np.array(loaded_weights_model_results))"
      ],
      "metadata": {
        "id": "GLkjEHs7cl1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FINE TUNING MODEL_3**\n",
        "\n",
        "High-level example of fine-tuning an EfficientNet model. Bottom layers (layers closer to the input data) stay frozen where as top layers (layers closer to the output data) are updated during training.\n",
        "\n",
        "So far our saved model has been trained using feature extraction transfer learning for 5 epochs on 10% of the training data and data augmentation.\n",
        "\n",
        "This means all of the layers in the base model (EfficientNetV2B0) were frozen during training.\n",
        "\n",
        "For our next experiment we're going to switch to fine-tuning transfer learning. This means we'll be using the same base model except we'll be unfreezing some of its layers (ones closest to the top) and running the model for a few more epochs.\n",
        "\n",
        "The idea with fine-tuning is to start customizing the pre-trained model more to our own data.\n",
        "\n",
        "- 🔑 Note: Fine-tuning usually works best after training a feature extraction model for a few epochs and with large amounts of data. For more on this, check out Keras' guide on Transfer learning & fine-tuning."
      ],
      "metadata": {
        "id": "gFv-6Ep_fWos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.layers"
      ],
      "metadata": {
        "id": "PMYsXDlPflK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#are these trainable layers\n",
        "for layer in model_2.layers:\n",
        "  print(f\"{layer}  - {layer.trainable}\")"
      ],
      "metadata": {
        "id": "kG-PsZPMgnh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've got an input layer, a Sequential layer (the data augmentation model), a Functional layer (EfficientNetV2B0), a pooling layer and a Dense layer (the output layer)."
      ],
      "metadata": {
        "id": "iZz9WwfPhsQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.summary()"
      ],
      "metadata": {
        "id": "JpUT_nI4htCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "t looks like all of the layers in the efficientnetv2-b0 layer are frozen. We can confirm this using the trainable_variables attribute."
      ],
      "metadata": {
        "id": "9QtKDfMYilAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.layers[2].trainable_variables #empty array suggests that this layer has no trainable layers\n",
        "#model_2.layers[2] is the same as the base_model it is the functional layer i.e our efficientnet_v2"
      ],
      "metadata": {
        "id": "Zysy8O-pjIGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which layers are tuneable (trainable)\n",
        "for layer_number, layer in enumerate(model_2.layers[2].layers):\n",
        "  print(layer_number, layer.name, layer.trainable)"
      ],
      "metadata": {
        "id": "Ayh-84MqjY42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to fine-tune the base model to our own data, we're going to unfreeze the top 10 layers and continue training our model for another 5 epochs.\n",
        "\n",
        "This means all of the base model's layers except for the last 10 will remain frozen and untrainable. And the weights in the remaining unfrozen layers will be updated during training.\n",
        "\n",
        "Ideally, we should see the model's performance improve."
      ],
      "metadata": {
        "id": "CJZUCtvhjrXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.layers[2].trainable=True\n",
        "for layer_num,layer in enumerate(model_2.layers[2].layers):\n",
        "  print(f\"{layer.name} - {layer.trainable}\")"
      ],
      "metadata": {
        "id": "jQ5Y30vGjr7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#freeze all layers except last 10\n",
        "for layer in model_2.layers[2].layers[:-10]:\n",
        "  layer.trainable=False\n",
        "\n",
        "# Recompile the whole model (always recompile after any adjustments to a model)\n",
        "model_2.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # lr is 10x lower than before for fine-tuning\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "YGka3SLinOYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now lets check the layers which are trainabe and which are not\n",
        "for layer in model_2.layers[2].layers:\n",
        "  print(f\"{layer.name} - {layer.trainable}\")"
      ],
      "metadata": {
        "id": "Gs_QQhaQnvLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice! It seems all layers except for the last 10 are frozen and untrainable.\n",
        "\n",
        " This means only the last 10 layers of the base model along with the output layer will have their weights updated during training."
      ],
      "metadata": {
        "id": "8jKTmDVNoHh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our case, we're using the exact same loss, optimizer and metrics as before, except this time the learning rate for our optimizer will be 10x smaller than before (0.0001 instead of Adam's default of 0.001).\n",
        "\n",
        "We do this so the model doesn't try to overwrite the existing weights in the pretrained model too fast. In other words, we want learning to be more gradual.\n",
        "\n",
        "🔑 Note: There's no set standard for setting the learning rate during fine-tuning, though reductions of 2.6x-10x+ seem to work well in practice."
      ],
      "metadata": {
        "id": "HwaqCZTzoKwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(model_2.trainable_variables)"
      ],
      "metadata": {
        "id": "F6-ojMqKoSsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wonderful, it looks like our model has a total of 12 trainable variables, the last 10 layers of the base model and the weight and bias parameters of the Dense output layer.\n",
        "\n",
        "Time to fine-tune!\n",
        "\n",
        "We're going to continue training on from where our previous model finished. Since it trained for 5 epochs, our fine-tuning will begin on the epoch 5 and continue for another 5 epochs.\n",
        "\n",
        "To do this, we can use the initial_epoch parameter of the fit() method. We'll pass it the last epoch of the previous model's training history (history_10_percent_data_aug.epoch[-1])."
      ],
      "metadata": {
        "id": "RD7Q6Is3pWtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine tune for another 5 epochs\n",
        "fine_tune_epochs = initial_epochs + 5\n",
        "\n",
        "# Refit the model (same as model_2 except with more trainable layers)\n",
        "history_fine_10_percent_data_aug = model_2.fit(train_data_10_percent,\n",
        "                                               epochs=fine_tune_epochs,\n",
        "                                               validation_data=test_data,\n",
        "                                               initial_epoch=history_10_percent_data_aug.epoch[-1], # start from previous last epoch\n",
        "                                               validation_steps=int(0.25 * len(test_data)))"
      ],
      "metadata": {
        "id": "1XY57Eu_qgsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_fine_tune_10_percent=model_2.evaluate(test_data)"
      ],
      "metadata": {
        "id": "fewlLf9pueL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_10_percent_data_aug"
      ],
      "metadata": {
        "id": "ZlAnhsZqwDgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **hence we get 3% more accuracy because of unfreezing top 10 layers and fine tuning**"
      ],
      "metadata": {
        "id": "oVle3rxfwMx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def compare_historys(original_history, new_history, initial_epochs=5):\n",
        "    \"\"\"\n",
        "    Compares two model history objects.\n",
        "    \"\"\"\n",
        "    # Get original history measurements\n",
        "    acc = original_history.history[\"accuracy\"]\n",
        "    loss = original_history.history[\"loss\"]\n",
        "\n",
        "    print(len(acc))\n",
        "\n",
        "    val_acc = original_history.history[\"val_accuracy\"]\n",
        "    val_loss = original_history.history[\"val_loss\"]\n",
        "\n",
        "    # Combine original history with new history\n",
        "    total_acc = acc + new_history.history[\"accuracy\"]\n",
        "    total_loss = loss + new_history.history[\"loss\"]\n",
        "\n",
        "    total_val_acc = val_acc + new_history.history[\"val_accuracy\"]\n",
        "    total_val_loss = val_loss + new_history.history[\"val_loss\"]\n",
        "\n",
        "    print(len(total_acc))\n",
        "    print(total_acc)\n",
        "\n",
        "    # Make plots\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(total_acc, label='Training Accuracy')\n",
        "    plt.plot(total_val_acc, label='Validation Accuracy')\n",
        "    plt.plot([initial_epochs-1, initial_epochs-1],\n",
        "              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(total_loss, label='Training Loss')\n",
        "    plt.plot(total_val_loss, label='Validation Loss')\n",
        "    plt.plot([initial_epochs-1, initial_epochs-1],\n",
        "              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "jCn9RmLSwpyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "compare_historys(original_history=history_10_percent_data_aug,\n",
        "                 new_history=history_fine_10_percent_data_aug,\n",
        "                 initial_epochs=5)"
      ],
      "metadata": {
        "id": "seesCgm0yK4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MODEL 4 WITH FINE TUNING AND 100% DATA AND PREPROCESSING**"
      ],
      "metadata": {
        "id": "RYgwEe9goaCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip"
      ],
      "metadata": {
        "id": "LEC3yOfGoodK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b7b2a1b-15c2-452d-e31a-67aff3854e47"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-14 12:26:10--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.212.207, 173.194.213.207, 173.194.215.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.212.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 519183241 (495M) [application/zip]\n",
            "Saving to: ‘10_food_classes_all_data.zip.1’\n",
            "\n",
            "10_food_classes_all 100%[===================>] 495.13M  46.7MB/s    in 10s     \n",
            "\n",
            "2024-01-14 12:26:20 (49.6 MB/s) - ‘10_food_classes_all_data.zip.1’ saved [519183241/519183241]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_ref=zipfile.ZipFile(\"10_food_classes_all_data.zip\")\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "BqrOvATAouFt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for dirpath,dirname,filename in os.walk(\"/content/10_food_classes_all_data\"):\n",
        "  print(f\"Found {len(dirname)} directories and {len(filename)} images in {dirpath}\")"
      ],
      "metadata": {
        "id": "UDjHX7eNo9gw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7165327-5503-41d6-9f58-36cccee46c2a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 directories and 0 images in /content/10_food_classes_all_data\n",
            "Found 10 directories and 0 images in /content/10_food_classes_all_data/test\n",
            "Found 0 directories and 250 images in /content/10_food_classes_all_data/test/hamburger\n",
            "Found 0 directories and 250 images in /content/10_food_classes_all_data/test/chicken_wings\n",
            "Found 0 directories and 250 images in /content/10_food_classes_all_data/test/ramen\n",
            "Found 0 directories and 250 images in /content/10_food_classes_all_data/test/sushi\n",
            "Found 0 directories and 250 images in /content/10_food_classes_all_data/test/grilled_salmon\n",
            "Found 0 directories and 250 images in /content/10_food_classes_all_data/test/chicken_curry\n",
            "Found 0 directories and 250 images in /content/10_food_classes_all_data/test/steak\n",
            "Found 0 directories and 250 images in /content/10_food_classes_all_data/test/pizza\n",
            "Found 0 directories and 250 images in /content/10_food_classes_all_data/test/ice_cream\n",
            "Found 0 directories and 250 images in /content/10_food_classes_all_data/test/fried_rice\n",
            "Found 10 directories and 0 images in /content/10_food_classes_all_data/train\n",
            "Found 0 directories and 750 images in /content/10_food_classes_all_data/train/hamburger\n",
            "Found 0 directories and 750 images in /content/10_food_classes_all_data/train/chicken_wings\n",
            "Found 0 directories and 750 images in /content/10_food_classes_all_data/train/ramen\n",
            "Found 0 directories and 750 images in /content/10_food_classes_all_data/train/sushi\n",
            "Found 0 directories and 750 images in /content/10_food_classes_all_data/train/grilled_salmon\n",
            "Found 0 directories and 750 images in /content/10_food_classes_all_data/train/chicken_curry\n",
            "Found 0 directories and 750 images in /content/10_food_classes_all_data/train/steak\n",
            "Found 0 directories and 750 images in /content/10_food_classes_all_data/train/pizza\n",
            "Found 0 directories and 750 images in /content/10_food_classes_all_data/train/ice_cream\n",
            "Found 0 directories and 750 images in /content/10_food_classes_all_data/train/fried_rice\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir=\"10_food_classes_all_data/train\"\n",
        "test_dir=\"10_food_classes_all_data/test\""
      ],
      "metadata": {
        "id": "gfVthkDJprGw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "img_size=(224,224)\n",
        "train_data_10_full_class=tf.keras.preprocessing.image_dataset_from_directory(train_dir,\n",
        "                                                                             image_size=img_size,\n",
        "                                                                             label_mode=\"categorical\")\n",
        "test_data_10_full_class=tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n",
        "                                                                             image_size=img_size,\n",
        "                                                                             label_mode=\"categorical\")"
      ],
      "metadata": {
        "id": "8pZReF6kpyVV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a8e2beb-2192-4b8c-a4b9-1edeef69f157"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7500 files belonging to 10 classes.\n",
            "Found 2500 files belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "to train model_4 we need to revert model_2 to its feature extraction weights\n",
        "\n",
        "More specifically, we're trying to measure:\n",
        "\n",
        "- Experiment 3 (previous one) - model_2 with 10 layers fine-tuned for 5 more epochs on 10% of the data.\n",
        "- Experiment 4 (this one) - model_2 with layers fined-tuned for 5 more epochs on 100% on the data."
      ],
      "metadata": {
        "id": "Zfr4Zh51tCVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lets load the weights after feature extraction\n",
        "model_2.load_weights(checkpoint_path)"
      ],
      "metadata": {
        "id": "6tKv5Y-jvGYd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62ae5f43-beb1-4a8d-d1d9-662df9da5a70"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7844a5c7e410>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### this model has been loaded from the point after we did feature extraction and before the fine tuning, so now we can fine tune this with 100% of the data."
      ],
      "metadata": {
        "id": "DcII-SdZyC4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for layer_num,layer in enumerate(model_2.layers[2].layers):\n",
        "  print(layer_num , layer.name , layer.trainable)"
      ],
      "metadata": {
        "id": "QA6Nhh7Gy1yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze all the layers in model_2's base_model\n",
        "model_2_base_model = model_2.layers[2]\n",
        "model_2_base_model.trainable = True\n",
        "\n",
        "#freeze all except top 10 layers\n",
        "for layer in (model_2_base_model.layers[:-10]):\n",
        "  layer.trainable=False"
      ],
      "metadata": {
        "id": "kCvzVLOB5WQM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer_num,layer in enumerate(model_2_base_model.layers):\n",
        "  print(layer_num,layer,layer.trainable)"
      ],
      "metadata": {
        "id": "YRvgsQMX6OXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compile the model again\n",
        "model_2.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # lr is 10x lower than before for fine-tuning\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# Continue to train and fine-tune the model to our data\n",
        "fine_tune_epochs = initial_epochs + 5\n",
        "\n",
        "history_fine_10_classes_full = model_2.fit(train_data_10_full_class,\n",
        "                                           epochs=fine_tune_epochs,\n",
        "                                           initial_epoch=history_10_percent_data_aug.epoch[-1], #start training from the last epoch of the last time we fit our model i.e after feature extraction's last epoch\n",
        "                                           validation_data=test_data,\n",
        "                                           validation_steps=int(0.25 * len(test_data)))"
      ],
      "metadata": {
        "id": "aL7C6d9k67Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "results_fine_tune_full_data = model_2.evaluate(test_data)\n",
        "results_fine_tune_full_data,results_fine_tune_10_percent"
      ],
      "metadata": {
        "id": "bcs0koi2876Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How did fine-tuning go with more data?\n",
        "compare_historys(original_history=history_10_percent_data_aug,\n",
        "                 new_history=history_fine_10_classes_full,\n",
        "                 initial_epochs=5)\n",
        ""
      ],
      "metadata": {
        "id": "_LwyBszc9Bru"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}